---
title: "Mobile phone price classifiaction"
author: "Group 1"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Mobile Phone Price Classification
    
## Business Understanding

The increasing use of smartphones is evident in the world today. For mobile dealers, an effective pricing strategy is essential for continued sales success. It is important to have a carefully planned pricing strategy.

Smartphone prices can be affected by a wide range of factors, such as brand and the length of time a particular device has been on the market. The ability to use a device internationally can also affect its price. If a device has a high-resolution camera and a lot of storage capacity, it will likely cost more than a unit without less notable capabilities. Those individuals who are willing to enter a contract, however, may be able to get a smartphone loaded with desirable features at a bargain price.

## Defining the question

Basing on the phones features we build a machine learning model that predicts  its price. 


## Defining the metrics for metrics for Success

Build a machine learning model that predicts the price class of a mobile phone with an accuracy of above 75%.

## Experimental Design 

The following approach will be taken to achieve our goal:
1) Loading the data

2) Data cleaning

3) Exploratory Data analysis

4) Feature Engineering

5) Data modelling using Supervised learning algorithms

6) Challenging the solution Using unsupervised learning.

7) Conclusions and reccomendations.


## Reading data

```{r}
#Loading the libraries
library(data.table)
library(tidyverse)

```

```{r}
phone_df <- read.csv("C:\\Users\\pharyz\\Downloads\\train (2).csv", header = TRUE) 
phone_df
```
```{r}
#Checking if there null values
colSums(is.na(phone_df))

```

The dataset has no null values

```{r}
#Checking if there are duplicated entries
dupli <- phone_df[duplicated(phone_df),]
dupli
```

The dataset has no duplicates 

# Checking For outliers
```{r}
# Getting the nuerical variables  
num_columns <- c("battery_power", "clock_speed", "fc", "int_memory", "m_dep", "mobile_wt","n_cores", "pc", "px_height", "px_width", "ram", "sc_h", "sc_w", "talk_time")
num_df<- phone_df[,num_columns]
head(num_df)

# Getting the categorical variables
cat_df <- phone_df[c(2,4,6,18,19,20,21)]
head(cat_df)

```

```{r}
# Plotting boxplots to check for outliers
op <- par(mfrow=c(1,1))  # to put boxplots side by side
lapply(seq(num_df), function(x) 
  boxplot(x=num_df[[x]], xlab=names(num_df)[x], main=paste("Boxplot",colnames(num_df[,x]))))
par(op) 

```
We have outliers in two columns namely front camera megapixels, phone height. Front camera megapixels had three outliers and phone height with one outlier.

# Anomaly Detection

```{r}
summary(num_df)
```
From the summary we note that there are no anomalies that can be detected at this point.

# EXPLORATORY DATA ANALYSIS
#uNIVARIATE ANALYSIS

```{r}
#Converting the encoded columns back to names
phone_df$price_range <- factor(phone_df$price_range, levels = c(0,1,2,3),
                       labels = c("low cost", "medium cost", "high cost", "very high cost"))

phone_df$blue <- factor(phone_df$blue, levels = c(0,1),
                                  labels = c("not", "yes"))
phone_df$dual_sim <- factor(phone_df$dual_sim, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$four_g <- factor(phone_df$four_g, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$three_g <- factor(phone_df$three_g, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$touch_screen <- factor(phone_df$touch_screen, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$wifi <- factor(phone_df$wifi, levels = c(0,1),
                               labels = c("not", "yes"))
```

```{r}
#plotting price range 
library(ggplot2)
ggplot(data=phone_df, aes(x = price_range)) +
  geom_bar()

```
Dataset has the same amount in each class that consist of low cost, medium cost, high cost, and very high cost.


```{r}
op <- par(mfrow=c(1,1))  # to put histograms side by side
lapply(seq(num_df), function(x) 
  hist(x=num_df[[x]], xlab=names(num_df)[x], main=paste("Histogram",colnames(num_df[,x]))))
par(op) 
```

* Most phones are between 500-600 mAh and the distribution is not normal
* Most phones had the lowest clockspeed between 0.5-.066 Ghz
* Most phones either had no front camera or the camera had the lowest megapixels of between 0-2
* Most phones have between 5-10 gb of internal memory
* Most phones have a depth of between 0.1-0.2cm
* Most phones have a weight of between 80 - 90 grams
* Most phones have 4 cores 
* Most phones have a primary cameras had either no primary camera or had primar cameras with megapixels of up to 2 
* Most phones had a pixel resolution height of between 200 - 400. The distribution is left-skewed
* Most phones had a pixel resolution height of between 800-900
* Most phones had a ram of 2000 - 2500 Mbs 
* Most phones had a screen height of between 5-6 cm


 
# BIVARIATE DATA ANALYSIS


```{r}
#Plotting four_G and price range 

ggplot(phone_df, aes(x=price_range, fill = four_g)) +
  geom_bar(position = "dodge")
```

```{r}
#Plotting dual sim and price range 
library(ggplot2)

ggplot(phone_df, aes(x=price_range, fill = dual_sim)) +
  geom_bar(position = "dodge")
```
  * low cost mobile are bought equally with without dual sim.
  * Medium cost mobiles with dual sim are bought more than without.
  * High cost mobile are bought equally with without dual sim.
  * Very high cost mobile with dula sim are bought more that those without.
  
```{r}
#Ploting mobiles with wifi aganist price range 
ggplot(phone_df, aes(x=price_range, fill = wifi)) +
  geom_bar(position = "dodge")
```
  * low cost mobile with wifi have a small diffence on how they are bought.
  * Medium cost mobiles with wifi have a small diffence on how they are bought.
  * High cost mobile with wifi have a small diffence on how they are bought.
  * Very high cost mobile with with wife are bought more that those without.
  
```{r}
#Ploting mobiles that are touch screen aganist price range 
ggplot(phone_df, aes(x=price_range, fill = touch_screen)) +
  geom_bar(position = "dodge")
```
  * low cost mobiles that are touch screened are bought more than those that not touch screened.
  * medium cost mobiles that are touch screened are bought more than those that not touch            screened.
  * high cost mobiles that are not touch screened are bought more than those that are touch         screened.
  * Very high cost mobiles that are touch screened are bought more than those that are not touch    screened.
  
  
```{r}
#Ploting mobiles that are three_g aganist price range 
ggplot(phone_df, aes(x=price_range, fill = three_g)) +
  geom_bar(position = "dodge")
```

 
  * Mobiles that are three_g are bought less than though that are not three_g across
  all price ranges.
 
```{r}
#Plotting a scatter plot between battery power and clock speed 
plot(phone_df$battery_power, phone_df$clock_speed, xlab="Battery power", ylab="clock speed")

```

There is no correlation battery power and clock speed.



```{r}
# Plotting scatter plot of phone weight and battery power
plot(num_df$battery_power, num_df$mobile_wt, xlab="Battery power", ylab="Phone weight")

```
Battery power and phone weight have no correlation

```{r}
# Plotting scatter plot of RAM and internal memory
plot(num_df$ram, num_df$int_memory, xlab="ram", ylab="Internal Memory")
```

There is no correlation between ram and internal memory

```{r}
#Finding the correlation of the numerical columns
library("dplyr") 
library(corrplot)


#Assigning m to the correlation
# correlation matrix
M<-cor(num_df)

corrplot(M, method="number", order = "hclust")


```
* There is a high positive correlation front camera megapixels and primary camera megapixels
* there is a moderately positive correlation between phone height and phone width

# FEATURE ENGINEERING
# Label encoding columns

```{r}
# Import the library for label encoding

library(superml)

```

```{r}
# Introduce the label encoder object
label <- LabelEncoder$new()

#Label encoding the columns
phone_df$price_range <-label$fit_transform(phone_df$price_range)
phone_df$blue <-label$fit_transform(phone_df$blue)
phone_df$dual_sim <-label$fit_transform(phone_df$dual_sim)
phone_df$four_g <-label$fit_transform(phone_df$four_g)
phone_df$three_g <-label$fit_transform(phone_df$three_g)
phone_df$touch_screen <-label$fit_transform(phone_df$touch_screen)
phone_df$wifi <-label$fit_transform(phone_df$wifi)

#Previewing the head
head(phone_df)
```


# Dimensionailty reduction using LDA

Since LDA assumes a normal distribution we first nomalising the data.

```{r}
#define Min-Max normalization function
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
  }

#apply Min-Max normalization to numerical columns
phone_norm <- as.data.frame(lapply(num_df, min_max_norm))

#view first six rows of normalized iris dataset
head(phone_norm)
     

```


```{r}
#add the target column to our normalized dataset
phone_norm$price_range <- phone_df$price_range

#view first six rows of data
head(phone_norm)

```


```{r}
# LINEAR DISCRIMINANT ANALYSIS
# Importing the necessary libraries
  
library(MASS)
library(tidyverse)
library(caret)
theme_set(theme_classic())
  
  
# Split the data into training (80%) and test set (20%)
set.seed(123)
training_data <- phone_df$price_range %>% 
            createDataPartition(p = 0.8, list = FALSE)
train_data <- phone_df[training_data, ]
test_data <- phone_df[-training_data, ]

head(train_data)
head(test_data)
```
```{r}
#Scaling the data
prep <- train_data %>% 
  preProcess(method = c("center", "scale"))
  
# Transform the data using the estimated parameters
train_transform <- prep %>% predict(train_data)
test_transform <- prep %>% predict(test_data)
  
# Fit the model
lda_model <- lda(price_range~., data = train_transform)
  
# Make predictions
lda_pred <- lda_model %>% predict(test_transform)

```

```{r}
# Model accuracy
mean(lda_pred$class==test_transform$price_range)
  
lda_model <- lda(price_range~., data = train_transform)
lda_model

```

FRom the LDA results we note that LD1 takes 99.70% of the data  so we are going to use it for modelling.


# SUPERVISED LEARNING 
# 1) LOGISTICS REGRESSION
```{r}
# Loading package
library(caTools)
library(ROCR)

```


```{r}
# Loading package
library(caTools)
library(ROCR) 
library(nnet)  


# Splitting dataset

#split <- sample.split(phone_df, SplitRatio = 0.8)
#split
   
#train_log <- subset(phone_df, split == "TRUE")
#test_log <- subset(phone_df, split == "FALSE")

#wt <- phone_df[c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)]
   
# Training model
#logistic_model <- glm(`price_range` ~ ., 
#                      data = train_log, 
#                      family = 'multiclass')
#logistic_model
#model <- nnet::multinom(price_range ~., data = train_log)  
# Summary
#summary(logistic_model)

```
```{r}
#require(foreign)
#require(ggplot2)
#require(MASS)
#require(Hmisc)
#require(reshape2)
#train_log$price_range <- factor(train_log$price_range)

#Build ordinal logistic regression model
#model1= polr(price_range ~ ., data = train_log, Hess = TRUE)
#summary(model1)
```
```{r}
# Making the Confusion Matrix
#log_cm = table(test_log$`price_range`, model)
#confusionMatrix(log_cm)
```

# K NEAREST NEIGHBOUR

```{r}
# Loading package
library(e1071)
library(class)
library(caTools)
library(ROCR)   
  
# Splitting data into train
# and test data
split <- sample.split(phone_df, SplitRatio = 0.7)
train_knn <- subset(phone_df, split == "TRUE")
test_knn <- subset(phone_df, split == "FALSE")
  
# Feature Scaling
train_scale <- scale(train_knn[,c(1,3,5,7,8,9,10,11,12,13,14,15,16,17)])
test_scale <- scale(test_knn[,c(1,3,5,7,8,9,10,11,12,13,14,15,16,17)])
  
# Fitting KNN Model 
# to training dataset
classifier_knn <- knn(train = train_knn,
                      test = test_knn,
                      cl = train_knn$price_range,
                      k = 44)
classifier_knn
```

```{r}
# Making the Confusion Matrix
cm = table(test_knn$`price_range`, classifier_knn)
confusionMatrix(cm)
```
# Finding optimum K

```{r}
library(ISLR)
library(caret)
# Split the data:
indxTrain <- createDataPartition(y = phone_df$price_range,p = 0.75,list = FALSE)
training <- phone_df[indxTrain,]
testing <- phone_df[-indxTrain,]
# Run k-NN:
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knnFit <- train(price_range ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
knnFit

```

# using optimum k
```{r}
# Fitting KNN Model 
# to training dataset
classifier_knn2 <- knn(train = train_knn,
                      test = test_knn,
                      cl = train_knn$price_range,
                      k = 13)
classifier_knn2
```
```{r}
# Making the Confusion Matrix
cm2 = table(test_knn$`price_range`, classifier_knn2)
confusionMatrix(cm2)
```


# Support Vector Machine

```{r}
# Splitting the dataset into the Training set and Test set
library(caTools)
 
set.seed(123)
split = sample.split(phone_df$price_range, SplitRatio = 0.75)
 
train_set = subset(phone_df, split == TRUE)
test_set = subset(phone_df, split == FALSE)

head(train_set)
head(test_set)

```
```{r}
# Feature Scaling the numerical columns
train_set[-21] <- scale(train_set[-21])
test_set[-21] <- scale(test_set[-21])

# Previewing the scaled dataset
train_set
test_set
```

```{r}

# Fitting SVM to the Training set
library(e1071)
 
svm_classifier = svm(formula = price_range ~ .,
                 data = train_set,
                 type = 'C-classification',
                 kernel = 'linear')
```

```{r}
# Predicting the Test set results
y_pred = predict(svm_classifier, newdata = test_set[-21])
y_pred
```
```{r}
# Making the Confusion Matrix
cm_svm = table(test_set[, 21], y_pred)
confusionMatrix(cm_svm)
```
Polynomial kernel give a accuracy of 72%
Sigmoid kernel gives an accuracy of 89%
linear kernel gives accuracy of 96%
Radial basis kernel gives an accuracy of 86%






```{r}
# installing library ElemStatLearn
#install.packages("ElemStatLearn")
#library(ElemStatLearn)
 
# Plotting the training data set results
#set = train_set
#X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
#X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
 
#grid_set = expand.grid(X1, X2)
#colnames(grid_set) = c('battery_power', 'clock_speed')
#y_grid = predict(svm_classifier, newdata = grid_set)
 
#plot(set[, -21],
# #    main = 'SVM (Train set)',
#     xlab = 'battery power', ylab = 'clock speed',
#     xlim = range(X1), ylim = range(X2))
 
#contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
 
#points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
 
#points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```


# UNSUPPERVISED LEARNING
```{R}
# Scaling continuous variables
new_df <- scale(num_df,scale = TRUE)
head(new_df)
# removing price_range from categorical
cat_df <- cat_df[,-7]

#Merging the scaled and categorical dataset
new_mobile<- do.call(cbind, list(new_df, cat_df))
new_mobile

```

## T-SNE
```{r}
#Creating t-sne model 
library(caret)  
library(Rtsne)
phone_df$price_range<-as.factor(phone_df$price_range)
colors = rainbow(length(unique(phone_df$price_range)))
names(colors) = unique(phone_df$price_range)
tsne_df <- Rtsne(new_mobile, dims = 2, perplexity=50, verbose=TRUE, max_iter = 1000)
```
```{r}
#Plotting the t-sne
# Plotting our graph and closely examining the graph
plot(tsne_df$Y, t='n', main="tsne")
text(tsne_df$Y, labels=phone_df$price_range, col=colors[phone_df$price_range])
```


## Hierachical clustering
```{r}
#creating heirarchical model 

## Creating hierarchical cluster model, and assigning the result to the data used to create the #tsne
d <- dist(new_mobile, method = "euclidean")
res.hc <- hclust(d, method = "ward.D2" )


# Splitting the hierarchical model to 4 clusters
clust = cutree(res.hc, k=4)

```

```{r}
#Plot (Dendrogram) the clusters to get height 
plot(res.hc, cex = 0.6, hang = -1)
```

```{r}
# Table of the clusters for hierachical clustering
table(clust)
```

 Using hierarchical clustering we were able to deduce the four clusters were separated as seen above

## K-mean clustering 

```{r}
#Creating k mean cluster model assigning the result to the data used to create the tsne

df_kmeans <- kmeans(new_mobile,centers = 4)
```



```{r}
# Table of the clusters for k-means
table(df_kmeans$cluster)
# calculate silhouette
library(cluster)   
sil <- silhouette(df_kmeans$cluster, dist(new_mobile))

# plot silhouette
library(factoextra)
fviz_silhouette(sil)
```


```{r}

```

```{r}

```


